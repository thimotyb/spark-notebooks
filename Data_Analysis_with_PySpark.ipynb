{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kkSXsFae8Cct",
        "7AYS0MlUvfrI",
        "WNaboQo22HOW",
        "2fj4XecA5nX_",
        "fUjc4g4xHE7p",
        "D6Syynf5QCxB",
        "0k8qNqs2T2a0",
        "jzlxJnZ6bNcq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thimotyb/spark-notebooks/blob/main/Data_Analysis_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "#Data Analysis with PySpark in Google Colab\n",
        "\n",
        "PySpark is Python interface for Apache Spark. The primary use cases for PySpark are to work with huge amounts of data and for creating data pipelines.\n",
        "\n",
        "You don't need to work with big data to benefit from PySpark. I find that the SparkSQL is a great tool for performing routine data anlysis. Pandas can get slow and you may find yourself writing a lot of code for data cleaning whereas the same actions take much less code in SQL. Let's get started!\n",
        "\n",
        "See more here! http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "ec147d6a-14c3-4cdb-f516-c5d6c1a91c8b"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "#  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,370 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,590 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [54.3 kB]\n",
            "Fetched 18.8 MB in 5s (3,813 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=5a089b2637410234d70af640993157617519b71d5621d98796400e01b52ecc6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78b8841a30d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7c81f60acd8e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hcOCBgQo2Pqf",
        "outputId": "97b82dae-5fb6-4c7b-9bef-1af71f14f23c"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78b8841a30d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7c81f60acd8e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get more example data\n",
        "!git clone https://github.com/thimotyb/DataAnalysisWithPythonAndPySpark-Data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYX_v49bIXYR",
        "outputId": "6d75bd15-ecf3-48c0-e399-03094d64a995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataAnalysisWithPythonAndPySpark-Data'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71 (from 1)\u001b[K\n",
            "Receiving objects: 100% (71/71), 828.81 MiB | 26.43 MiB/s, done.\n",
            "Updating files: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv DataAnalysisWithPythonAndPySpark-Data /content/data"
      ],
      "metadata": {
        "id": "jg40eTP6IwfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Data Analysis Examples"
      ],
      "metadata": {
        "id": "GbnrZrIcJZ-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCount Example"
      ],
      "metadata": {
        "id": "9Yu2tpSDJl2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "book = spark.read.text(\"data/gutenberg_books/1342-0.txt\")\n",
        "\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
        ")\n",
        "\n",
        "words_nonull = words_clean.where(col(\"word\") != \"\")"
      ],
      "metadata": {
        "id": "zo8SK4fQeMUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_nonull.show(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOiTP37IJr4d",
        "outputId": "54959525-7c2b-4c5f-dd31-3b0ebfd90b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|        word|\n",
            "+------------+\n",
            "|         the|\n",
            "|     project|\n",
            "|   gutenberg|\n",
            "|       ebook|\n",
            "|          of|\n",
            "|       pride|\n",
            "|         and|\n",
            "|   prejudice|\n",
            "|          by|\n",
            "|        jane|\n",
            "|      austen|\n",
            "|        this|\n",
            "|       ebook|\n",
            "|          is|\n",
            "|         for|\n",
            "|         the|\n",
            "|         use|\n",
            "|          of|\n",
            "|      anyone|\n",
            "|    anywhere|\n",
            "|          at|\n",
            "|          no|\n",
            "|        cost|\n",
            "|         and|\n",
            "|        with|\n",
            "|      almost|\n",
            "|          no|\n",
            "|restrictions|\n",
            "|  whatsoever|\n",
            "|         you|\n",
            "|         may|\n",
            "|        copy|\n",
            "|          it|\n",
            "|        give|\n",
            "|          it|\n",
            "|        away|\n",
            "|          or|\n",
            "|          re|\n",
            "|          it|\n",
            "|       under|\n",
            "|         the|\n",
            "|       terms|\n",
            "|          of|\n",
            "|         the|\n",
            "|     project|\n",
            "|   gutenberg|\n",
            "|     license|\n",
            "|    included|\n",
            "|        with|\n",
            "|        this|\n",
            "+------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    spark.read.csv(\"./data/list_of_numbers/sample.csv\", header=True)\n",
        "    .withColumn(\n",
        "        \"new_column\", F.when(F.col(\"old_column\") > 10, 10).otherwise(0)\n",
        "    )\n",
        "    .where(\"old_column > 8\")\n",
        "    .groupby(\"new_column\")\n",
        "    .count()\n",
        "    .write.csv(\"updated_frequencies.csv\", mode=\"overwrite\")\n",
        ")"
      ],
      "metadata": {
        "id": "NfUYt0kzJwXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xl-mJ8PYMlrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}