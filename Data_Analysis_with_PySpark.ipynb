{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kkSXsFae8Cct",
        "7AYS0MlUvfrI",
        "WNaboQo22HOW",
        "2fj4XecA5nX_",
        "fUjc4g4xHE7p",
        "D6Syynf5QCxB",
        "0k8qNqs2T2a0",
        "jzlxJnZ6bNcq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thimotyb/spark-notebooks/blob/main/Data_Analysis_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "#Data Analysis with PySpark in Google Colab\n",
        "\n",
        "PySpark is Python interface for Apache Spark. The primary use cases for PySpark are to work with huge amounts of data and for creating data pipelines.\n",
        "\n",
        "You don't need to work with big data to benefit from PySpark. I find that the SparkSQL is a great tool for performing routine data anlysis. Pandas can get slow and you may find yourself writing a lot of code for data cleaning whereas the same actions take much less code in SQL. Let's get started!\n",
        "\n",
        "See more here! http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "007dad9d-4dc9-42d7-fc68-096aad4f0923"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "#  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,371 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,590 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Fetched 18.9 MB in 6s (2,998 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=7073e0404b4272e8f634f19e8f4678acf77396a1e670a51ebe93903a379a0531\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a6bd0177190>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f225feddc3eb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hcOCBgQo2Pqf",
        "outputId": "97b82dae-5fb6-4c7b-9bef-1af71f14f23c"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78b8841a30d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7c81f60acd8e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get more example data\n",
        "!git clone https://github.com/thimotyb/DataAnalysisWithPythonAndPySpark-Data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYX_v49bIXYR",
        "outputId": "0742270f-12fa-47dc-fc94-0bcc6eadde0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataAnalysisWithPythonAndPySpark-Data'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71 (from 1)\u001b[K\n",
            "Receiving objects: 100% (71/71), 828.81 MiB | 27.83 MiB/s, done.\n",
            "Updating files: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv DataAnalysisWithPythonAndPySpark-Data /content/data"
      ],
      "metadata": {
        "id": "jg40eTP6IwfR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Data Analysis Examples"
      ],
      "metadata": {
        "id": "GbnrZrIcJZ-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCount Example"
      ],
      "metadata": {
        "id": "9Yu2tpSDJl2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "book = spark.read.text(\"data/gutenberg_books/1342-0.txt\")\n",
        "\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
        ")\n",
        "\n",
        "words_nonull = words_clean.where(col(\"word\") != \"\")"
      ],
      "metadata": {
        "id": "zo8SK4fQeMUQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_nonull.show(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOiTP37IJr4d",
        "outputId": "31b4b615-3fb6-4ed6-a103-ae126bdca856"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|        word|\n",
            "+------------+\n",
            "|         the|\n",
            "|     project|\n",
            "|   gutenberg|\n",
            "|       ebook|\n",
            "|          of|\n",
            "|       pride|\n",
            "|         and|\n",
            "|   prejudice|\n",
            "|          by|\n",
            "|        jane|\n",
            "|      austen|\n",
            "|        this|\n",
            "|       ebook|\n",
            "|          is|\n",
            "|         for|\n",
            "|         the|\n",
            "|         use|\n",
            "|          of|\n",
            "|      anyone|\n",
            "|    anywhere|\n",
            "|          at|\n",
            "|          no|\n",
            "|        cost|\n",
            "|         and|\n",
            "|        with|\n",
            "|      almost|\n",
            "|          no|\n",
            "|restrictions|\n",
            "|  whatsoever|\n",
            "|         you|\n",
            "|         may|\n",
            "|        copy|\n",
            "|          it|\n",
            "|        give|\n",
            "|          it|\n",
            "|        away|\n",
            "|          or|\n",
            "|          re|\n",
            "|          it|\n",
            "|       under|\n",
            "|         the|\n",
            "|       terms|\n",
            "|          of|\n",
            "|         the|\n",
            "|     project|\n",
            "|   gutenberg|\n",
            "|     license|\n",
            "|    included|\n",
            "|        with|\n",
            "|        this|\n",
            "+------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    spark.read.csv(\"./data/list_of_numbers/sample.csv\", header=True)\n",
        "    .withColumn(\n",
        "        \"new_column\", F.when(F.col(\"old_column\") > 10, 10).otherwise(0)\n",
        "    )\n",
        "    .where(\"old_column > 8\")\n",
        "    .groupby(\"new_column\")\n",
        "    .count()\n",
        "    .write.csv(\"updated_frequencies.csv\", mode=\"overwrite\")\n",
        ")"
      ],
      "metadata": {
        "id": "NfUYt0kzJwXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PySpark SQL"
      ],
      "metadata": {
        "id": "BzXd7IaJh_fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "xl-mJ8PYMlrs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_grocery_list = [\n",
        "    [\"Banana\", 2, 1.74],\n",
        "    [\"Apple\", 4, 2.04],\n",
        "    [\"Carrot\", 1, 1.09],\n",
        "    [\"Cake\", 1, 10.99],\n",
        "]\n",
        "\n",
        "df_grocery_list = spark.createDataFrame(\n",
        "    my_grocery_list, [\"Item\", \"Quantity\", \"Price\"]\n",
        ")\n",
        "\n",
        "df_grocery_list.printSchema()"
      ],
      "metadata": {
        "id": "sVdQbz_DlPvT",
        "outputId": "96de9379-7585-4ff0-c61f-45e352df9cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Item: string (nullable = true)\n",
            " |-- Quantity: long (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grocery_list.select(\"Item\", \"Price\").where(\"Price > 2\").show()"
      ],
      "metadata": {
        "id": "ge91tFVWlbYj",
        "outputId": "83c65265-f9bb-4a65-b21e-24138544c7b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| Item|Price|\n",
            "+-----+-----+\n",
            "|Apple| 2.04|\n",
            "| Cake|10.99|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTORY = \"data/broadcast_logs\"\n",
        "logs = (\n",
        "    spark.read.csv(\n",
        "        os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
        "        sep=\"|\",\n",
        "        header=True,\n",
        "        inferSchema=True,\n",
        "        timestampFormat=\"yyyy-MM-dd\",\n",
        "    )\n",
        "    .drop(\"BroadcastLogID\", \"SequenceNO\")\n",
        "    .withColumn(\n",
        "        \"duration_seconds\",\n",
        "        (\n",
        "            F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
        "            + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
        "            + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
        "        ),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "lKOlEtJrwnLN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs.printSchema()"
      ],
      "metadata": {
        "id": "gQ5n876QxJjX",
        "outputId": "780863be-b7d7-478b-9cc1-7dd1a563bb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- LogServiceID: integer (nullable = true)\n",
            " |-- LogDate: date (nullable = true)\n",
            " |-- AudienceTargetAgeID: integer (nullable = true)\n",
            " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
            " |-- CategoryID: integer (nullable = true)\n",
            " |-- ClosedCaptionID: integer (nullable = true)\n",
            " |-- CountryOfOriginID: integer (nullable = true)\n",
            " |-- DubDramaCreditID: integer (nullable = true)\n",
            " |-- EthnicProgramID: integer (nullable = true)\n",
            " |-- ProductionSourceID: integer (nullable = true)\n",
            " |-- ProgramClassID: integer (nullable = true)\n",
            " |-- FilmClassificationID: integer (nullable = true)\n",
            " |-- ExhibitionID: integer (nullable = true)\n",
            " |-- Duration: string (nullable = true)\n",
            " |-- EndTime: string (nullable = true)\n",
            " |-- LogEntryDate: date (nullable = true)\n",
            " |-- ProductionNO: string (nullable = true)\n",
            " |-- ProgramTitle: string (nullable = true)\n",
            " |-- StartTime: string (nullable = true)\n",
            " |-- Subtitle: string (nullable = true)\n",
            " |-- NetworkAffiliationID: integer (nullable = true)\n",
            " |-- SpecialAttentionID: integer (nullable = true)\n",
            " |-- BroadcastOriginPointID: integer (nullable = true)\n",
            " |-- CompositionID: integer (nullable = true)\n",
            " |-- Producer1: string (nullable = true)\n",
            " |-- Producer2: string (nullable = true)\n",
            " |-- Language1: integer (nullable = true)\n",
            " |-- Language2: integer (nullable = true)\n",
            " |-- duration_seconds: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIe_cTijxzTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}